{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Keywords using LDA Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gieba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5c0a03df4fdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgieba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gieba'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import gieba\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from bokeh.plotting import figure, show, output_notebook, save\n",
    "from bokeh.models import HoverTool, value, LabelSet, Legend, ColumnDataSource\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_dataset(filename):\n",
    "    \"\"\"\n",
    "    Read JSON dataset and return the job requirements\n",
    "    \n",
    "    Returns:\n",
    "        job requirements\n",
    "    \"\"\"\n",
    "    dataset = pd.read_json(filename)\n",
    "    \n",
    "    return dataset['requirement'] + ' ' + dataset['requirement_others']\n",
    "\n",
    "\n",
    "def tokenizer(doc):\n",
    "    return gieba.cut(doc, cut_all=False)\n",
    "\n",
    "\n",
    "def get_chinese_stopwords():\n",
    "    \"\"\"\n",
    "    Get Chinese stopwords\n",
    "    \"\"\"\n",
    "    with open('./中文停用词表.txt') as f:\n",
    "        lines = f.readlines()\n",
    "        stopwords_chinese = [w.strip() for w in lines]\n",
    "        \n",
    "    return stopwords_chinese\n",
    "\n",
    "    \n",
    "def preprocess(raw_docs):\n",
    "    \"\"\"\n",
    "    Normalize, tokenize, remove stopwords, use custom dictionary\n",
    "    \n",
    "    Args:\n",
    "        raw_docs (list(str)):\n",
    "        \n",
    "    Returns:\n",
    "        docs (list(list(str))): list of tokens in a document\n",
    "    \"\"\"\n",
    "    \n",
    "    docs = []\n",
    "    \n",
    "    # define customized stopwords\n",
    "    stopwords_custom = ['•', '與', '★', '●', '（', '’', '－', '✦', '◆', '◼', '✪', \n",
    "                        '※', '⁺', '', '', '·', '‧', '・', '）', '○', '】', '【', '✓', '']\n",
    "    \n",
    "    stopwords_chinese = get_chinese_stopwords()\n",
    "    \n",
    "    for d in tqdm(raw_docs):\n",
    "        # Normalize English words\n",
    "        d = d.lower()\n",
    "        \n",
    "        tokens = []\n",
    "        \n",
    "        for t in tokenizer(d):  \n",
    "            # Strip English punctuations\n",
    "            t = gensim.parsing.preprocessing.strip_punctuation(t)\n",
    "            \n",
    "            # Remove numbers\n",
    "            t = gensim.parsing.preprocessing.strip_numeric(t)\n",
    "            \n",
    "            t = t.strip()\n",
    "            \n",
    "            if t is '':\n",
    "                continue\n",
    "                \n",
    "            if t not in stopwords_custom:\n",
    "                if t not in stopwords_chinese:\n",
    "                    if t not in gensim.parsing.preprocessing.STOPWORDS:\n",
    "                        tokens.append(t)\n",
    "                    \n",
    "        docs.append(tokens)\n",
    "        \n",
    "    return docs\n",
    "\n",
    "\n",
    "def get_dictionary(docs):\n",
    "    return gensim.corpora.Dictionary(docs)\n",
    "\n",
    "\n",
    "def get_corpus_bow(docs, dictionary):\n",
    "    \"\"\"\n",
    "    Get corpus with format of BOW\n",
    "    \n",
    "    Args:\n",
    "        docs (list(list(str))): list of list of string token\n",
    "        dictionary (gensim.corpora.Dictionary): dictionary\n",
    "        \n",
    "    Returns:\n",
    "        bows (list(list(tuple(token_id, num_tokens))))\n",
    "    \"\"\"\n",
    "    bows = [dictionary.doc2bow(d) for d in docs]\n",
    "    \n",
    "    return bows\n",
    "\n",
    "\n",
    "def get_corpus_tfidf(bows):\n",
    "    \"\"\"\n",
    "    Get corpus based on TF-IDF and TF-IDF model\n",
    "    \n",
    "    Args:\n",
    "        bows (list(list(tuple(token_id, num_tokens)))): BOW\n",
    "        \n",
    "    Returns:\n",
    "        corpus_tfidf (gensim.interfaces.TransformedCorpus): corpus class\n",
    "        tfidf_model: \n",
    "    \"\"\"\n",
    "    tfidf_model = gensim.models.TfidfModel(bows)\n",
    "    corpus_tfidf = tfidf_model[bows]\n",
    "    \n",
    "    return corpus_tfidf, tfidf_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/996 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gieba' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d5c20e466d02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Normalize, tokenize, remove stopwords, use custom dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Create dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-76b290303396>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(raw_docs)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0;31m# Strip English punctuations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-76b290303396>\u001b[0m in \u001b[0;36mtokenizer\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgieba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcut_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gieba' is not defined"
     ]
    }
   ],
   "source": [
    "filename_dataset = '../crawler/employment_website_104/dataset/jobs_104.json'\n",
    "no_below = 6\n",
    "no_above = 0.1\n",
    "\n",
    "raw_docs = read_json_dataset(filename_dataset)\n",
    "\n",
    "# Normalize, tokenize, remove stopwords, use custom dictionary\n",
    "docs = preprocess(raw_docs)\n",
    "\n",
    "# Create dictionary\n",
    "dictionary = get_dictionary(docs)\n",
    "print('Size of dictionary before filtering out extreme words: {}'.format(len(dictionary)))\n",
    "\n",
    "# Remove common/rare words\n",
    "dictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "print('Size of dictionary after filtering out extreme words: {}'.format(len(dictionary)))\n",
    "\n",
    "# generate BOW corpus\n",
    "corpus_bow = get_corpus_bow(docs, dictionary)\n",
    "\n",
    "# generate TF-IDF corpus using BOW\n",
    "corpus_tfidf = get_corpus_tfidf(corpus_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Training Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train BOW model\n",
    "lda_model_bow = gensim.models.LdaModel(corpus_bow,\n",
    "                                       num_topics=num_topics,\n",
    "                                       id2word=dictionary,\n",
    "                                       passes=passes)\n",
    "\n",
    "# train TF-IDF model\n",
    "lda_model_tfidf = gensim.models.LdaModel(corpus_tfidf,\n",
    "                                        num_topics=num_topics,\n",
    "                                        id2word=dictionary,\n",
    "                                        passes=passes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_prefix_bow = './model_bow.lda'\n",
    "filename_preifx_tfidf = './model_tfidf.lda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lda_model_bow.save(filename_prefix_bow)\n",
    "lda_model_tfidf.save(filename_preifx_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_bow = gensim.models.LdaModel.load(filename_prefix_bow)\n",
    "lda_model_tfidf = gensim.models.LdaModel.load(filename_preifx_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Key Factors of Each Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_df(model):\n",
    "    init_values = [['']*num_topics]*10\n",
    "    keywords_df = pd.DataFrame(init_values, columns=['topic{}'.format(i) for i in range(num_topics)])\n",
    "\n",
    "\n",
    "    for i_topic, topic in model.show_topics(-1, formatted=False):\n",
    "        for i_word, (word, weight) in enumerate(topic):\n",
    "            keywords_df.iloc[i_word][i_topic] = word\n",
    "            \n",
    "    return keywords_df\n",
    "    \n",
    "    \n",
    "def plot_word_cloud(keywords_df):\n",
    "    num_topics = len(keywords_df.iloc[0])\n",
    "    \n",
    "    fig, axs = plt.subplots(num_topics)\n",
    "    fig.set_figheight(15)\n",
    "    fig.set_figwidth(15)\n",
    "\n",
    "    for i_topic in range(num_topics):\n",
    "        text = ' '.join(w for w in keywords_df.iloc[:, i_topic])\n",
    "        print('Topic {}: {}'.format(i_topic, text))\n",
    "        wordCloud = WordCloud(background_color='white').generate(text)\n",
    "\n",
    "        axs[i_topic].imshow(wordCloud)\n",
    "        axs[i_topic].axis('off')\n",
    "        axs[i_topic].set_title('Topic {}'.format(i_topic), size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_bow_df = get_keywords_df(lda_model_bow)\n",
    "plot_word_cloud(keywords_bow_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_df = get_keywords_df(lda_model_tfidf)\n",
    "plot_word_cloud(keywords_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify All Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax_prob(topic_probs):\n",
    "    \"\"\"\n",
    "    Get the index with max probability\n",
    "    \n",
    "    Args:\n",
    "        topic_probs (list of (topic_id, topic_probability)):\n",
    "        \n",
    "    Returns:\n",
    "        index (int): index with maximum probability\n",
    "    \"\"\"\n",
    "    max_value = -1\n",
    "    index_max_prob = 0\n",
    "    \n",
    "    for index, value in topic_probs:\n",
    "        if value > max_value:\n",
    "            max_value = value\n",
    "            index_max_prob = index\n",
    "            \n",
    "    return index_max_prob        \n",
    "    \n",
    "    \n",
    "def get_topic_probs(corpus, model):\n",
    "    \"\"\"\n",
    "    Get topics of corpus\n",
    "    \n",
    "    Args:\n",
    "        corpus\n",
    "        model\n",
    "    \"\"\"\n",
    "    topic_probs = []\n",
    "    \n",
    "    for c in corpus:\n",
    "        results = model[c]\n",
    "        \n",
    "        probs = []\n",
    "        for index, p in results:\n",
    "            probs.append(p)\n",
    "            \n",
    "        topic_probs.append(probs)\n",
    "        \n",
    "    return topic_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_bows = get_topic_probs(corpus_bows, lda_model_bow)\n",
    "topic_bows = np.argmax(prob_bows, axis=1)\n",
    "\n",
    "print(prob_bows[:5])\n",
    "print(topic_bows[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_tfidf = get_topic_probs(corpus_tfidf, lda_model_tfidf)\n",
    "topic_tfidf = np.argmax(prob_tfidf, axis=1)\n",
    "\n",
    "print(prob_tfidf[:5])\n",
    "print(topic_bows[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Probabilistic Results Using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_colors = {0: 'blue', 1: 'green', 2: 'yellow', 3: 'red', 4: 'skyblue', 5:'salmon', 6:'orange', 7:'maroon', 8:'crimson', 9:'black', 10:'gray'}\n",
    "\n",
    "labels = ['Topic {}'.format(i) for i in topic_tfidf]\n",
    "topic_colors = [cluster_colors[i] for i in topic_tfidf]\n",
    "\n",
    "settings = dict(x=X_tsne[:, 0],\n",
    "               y = X_tsne[:, 1],\n",
    "                label=labels,\n",
    "                color=topic_colors,\n",
    "               content=raw_docs[:])\n",
    "\n",
    "source = ColumnDataSource(settings)\n",
    "\n",
    "title = 'T-SNE visualization of Trump\\'s twitts'\n",
    "\n",
    "plot_lda = figure(plot_width=1000, plot_height=600,\n",
    "                     title=title, tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "                     x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "plot_lda.scatter(x='x', y='y', legend='label', source=source, \n",
    "                 color='color', alpha=0.8, size=10)#'msize', )\n",
    "\n",
    "hover = plot_lda.select(dict(type=HoverTool))\n",
    "hover.tooltips = {\"content\": \"@content\"}\n",
    "plot_lda.legend.location = \"top_left\"\n",
    "\n",
    "show(plot_lda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
