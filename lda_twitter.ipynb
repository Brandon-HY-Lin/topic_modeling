{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# download word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'abcnews-date-text.csv'\n",
    "filename = 'twitter_trump_2019_05.csv'\n",
    "raw_docs = pd.read_csv(filename, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "677"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>id_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Robert Mueller came to the Oval Office (along ...</td>\n",
       "      <td>05-30-2019 15:34:11</td>\n",
       "      <td>1134120831389392896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“Comey and Brennan are turning on each other.”...</td>\n",
       "      <td>05-30-2019 14:41:24</td>\n",
       "      <td>1134107544681455616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Congressman John Ratcliffe “The Trump Campaign...</td>\n",
       "      <td>05-30-2019 13:41:43</td>\n",
       "      <td>1134092525218590721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Russia Russia Russia! That’s all you heard at ...</td>\n",
       "      <td>05-30-2019 11:57:47</td>\n",
       "      <td>1134066371510378501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>....say he fought back against this phony crim...</td>\n",
       "      <td>05-30-2019 11:57:47</td>\n",
       "      <td>1134066372584062976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text           created_at  \\\n",
       "0  Robert Mueller came to the Oval Office (along ...  05-30-2019 15:34:11   \n",
       "1  “Comey and Brennan are turning on each other.”...  05-30-2019 14:41:24   \n",
       "2  Congressman John Ratcliffe “The Trump Campaign...  05-30-2019 13:41:43   \n",
       "3  Russia Russia Russia! That’s all you heard at ...  05-30-2019 11:57:47   \n",
       "4  ....say he fought back against this phony crim...  05-30-2019 11:57:47   \n",
       "\n",
       "                id_str  \n",
       "0  1134120831389392896  \n",
       "1  1134107544681455616  \n",
       "2  1134092525218590721  \n",
       "3  1134066371510378501  \n",
       "4  1134066372584062976  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_docs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Robert Mueller came to the Oval Office (along ...\n",
       "1    “Comey and Brennan are turning on each other.”...\n",
       "2    Congressman John Ratcliffe “The Trump Campaign...\n",
       "3    Russia Russia Russia! That’s all you heard at ...\n",
       "4    ....say he fought back against this phony crim...\n",
       "5    Russia Russia Russia! That’s all you heard at ...\n",
       "6    ....say he fought back against this phony crim...\n",
       "7    The Greatest Presidential Harassment in histor...\n",
       "8    I was not informed about anything having to do...\n",
       "9    Great show tonight @seanhannity you really get...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_docs['text'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "\n",
    "def preprocess_docs(raw_docs, num_docs=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        raw_docs: pandas.DataFrame\n",
    "\n",
    "    Returns:\n",
    "        list(list): return list of list\n",
    "    \"\"\"\n",
    "    if num_docs is None:\n",
    "        num_docs = len(raw_docs)\n",
    "        \n",
    "    docs = list()\n",
    "\n",
    "    stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    \n",
    "    stopwords_http = ['https', 'rt', 'amp']\n",
    "\n",
    "    for d in raw_docs['text'][:num_docs]:\n",
    "        \n",
    "#         print(d)\n",
    "        processed_tokens = list()\n",
    "        # normalize and tokenize\n",
    "        tokens = gensim.utils.simple_preprocess(d)\n",
    "    \n",
    "#         print(tokens)\n",
    "#         pdb.set_trace()\n",
    "\n",
    "        # lemmatize then stem\n",
    "        for t in tokens:\n",
    "            # remove stop words\n",
    "            if t not in gensim.parsing.preprocessing.STOPWORDS:\n",
    "                if t not in stopwords_http:\n",
    "    #             print(t)\n",
    "                    p_t = stemmer.stem(lemmatizer.lemmatize(t, pos='v'))\n",
    "                    processed_tokens.append(p_t)\n",
    "            \n",
    "    #             pdb.set_trace()\n",
    "\n",
    "        docs.append(processed_tokens)\n",
    "    \n",
    "    return docs\n",
    "\n",
    "\n",
    "docs = preprocess_docs(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "677\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))\n",
    "# print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(docs):\n",
    "    return gensim.corpora.Dictionary(docs)\n",
    "\n",
    "dictionary = create_dictionary(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to recover stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_word(stem, raw_docs):\n",
    "    index = raw_docs['text'].str.contains(stem, flags=re.IGNORECASE)\n",
    "    first_index = index.idxmax()\n",
    "    \n",
    "    matched_text = raw_docs['text'][first_index]\n",
    "\n",
    "    words = nltk.word_tokenize(matched_text)\n",
    "    \n",
    "    matched_word = None\n",
    "    \n",
    "    for w in words:\n",
    "        if stem in w.lower():\n",
    "            matched_word = w\n",
    "            break\n",
    "    \n",
    "    if matched_word is None:\n",
    "        return stem\n",
    "    \n",
    "    return matched_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bows = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bows[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_bow = gensim.models.LdaModel(bows,\n",
    "                                        num_topics=3,\n",
    "                                        id2word=dictionary,\n",
    "                                        passes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0: topic= 0.012*\"china\" + 0.010*\"great\" + 0.009*\"realdonaldtrump\" + 0.008*\"dollar\" + 0.007*\"billion\" + 0.007*\"state\" + 0.007*\"tariff\" + 0.007*\"presid\" + 0.006*\"mueller\" + 0.006*\"trump\"\n",
      "index 1: topic= 0.015*\"great\" + 0.008*\"time\" + 0.008*\"know\" + 0.008*\"democrat\" + 0.007*\"state\" + 0.006*\"work\" + 0.005*\"new\" + 0.005*\"presid\" + 0.004*\"dbongino\" + 0.004*\"job\"\n",
      "index 2: topic= 0.011*\"trump\" + 0.011*\"presid\" + 0.008*\"great\" + 0.008*\"countri\" + 0.008*\"china\" + 0.007*\"dbongino\" + 0.006*\"want\" + 0.005*\"peopl\" + 0.005*\"year\" + 0.005*\"deal\"\n"
     ]
    }
   ],
   "source": [
    "for index, topic in lda_model_bow.print_topics(-1):\n",
    "    print('index {}: topic= {}'.format(index, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "\n",
      "\tStems:\n",
      "\t\tchina\n",
      "\t\tgreat\n",
      "\t\trealdonaldtrump\n",
      "\t\tdollar\n",
      "\t\tbillion\n",
      "\t\tstate\n",
      "\t\ttariff\n",
      "\t\tpresid\n",
      "\t\tmueller\n",
      "\t\ttrump\n",
      "\n",
      "\n",
      "\tRecovered Words:\n",
      "\t\tChina\n",
      "\t\tGreatest\n",
      "\t\trealDonaldTrump\n",
      "\t\tDollar\n",
      "\t\tBillion\n",
      "\t\tStatement\n",
      "\t\ttariff\n",
      "\t\tPresidential\n",
      "\t\tMueller\n",
      "\t\tTrump\n",
      "\n",
      "\n",
      "Topic 1: \n",
      "\n",
      "\tStems:\n",
      "\t\tgreat\n",
      "\t\ttime\n",
      "\t\tknow\n",
      "\t\tdemocrat\n",
      "\t\tstate\n",
      "\t\twork\n",
      "\t\tnew\n",
      "\t\tpresid\n",
      "\t\tdbongino\n",
      "\t\tjob\n",
      "\n",
      "\n",
      "\tRecovered Words:\n",
      "\t\tGreatest\n",
      "\t\ttime\n",
      "\t\tknow\n",
      "\t\tDemocrats\n",
      "\t\tStatement\n",
      "\t\twork\n",
      "\t\tFoxNews\n",
      "\t\tPresidential\n",
      "\t\tdbongino\n",
      "\t\tjob\n",
      "\n",
      "\n",
      "Topic 2: \n",
      "\n",
      "\tStems:\n",
      "\t\ttrump\n",
      "\t\tpresid\n",
      "\t\tgreat\n",
      "\t\tcountri\n",
      "\t\tchina\n",
      "\t\tdbongino\n",
      "\t\twant\n",
      "\t\tpeopl\n",
      "\t\tyear\n",
      "\t\tdeal\n",
      "\n",
      "\n",
      "\tRecovered Words:\n",
      "\t\tTrump\n",
      "\t\tPresidential\n",
      "\t\tGreatest\n",
      "\t\tcountries\n",
      "\t\tChina\n",
      "\t\tdbongino\n",
      "\t\twanted\n",
      "\t\tpeople\n",
      "\t\tyears\n",
      "\t\tdeals\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, topic in lda_model_bow.show_topics(-1, formatted=False):\n",
    "    print('Topic {}: '.format(index))\n",
    "    \n",
    "    print('\\n\\tStems:')\n",
    "    \n",
    "    for word, weight in topic:\n",
    "        print('\\t\\t{}'.format(word), end='\\n')\n",
    "#         print('{} '.format(word))\n",
    "\n",
    "    print('\\n\\n\\tRecovered Words:')\n",
    "    \n",
    "    for word, weight in topic:\n",
    "        print('\\t\\t{}'.format(get_original_word(word, raw_docs)), end='\\n')\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bows2tfidf(bows):\n",
    "    tfidf = gensim.models.TfidfModel(bows)\n",
    "    corpus_tfidf = tfidf[bows]\n",
    "    \n",
    "    return corpus_tfidf\n",
    "\n",
    "corpus_tfidf = bows2tfidf(bows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.interfaces.TransformedCorpus"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaModel(corpus_tfidf,\n",
    "                                   num_topics=3,\n",
    "                                   id2word=dictionary,\n",
    "                                   passes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0: topic= 0.004*\"realdonaldtrump\" + 0.003*\"dbongino\" + 0.003*\"presid\" + 0.003*\"trump\" + 0.003*\"america\" + 0.003*\"china\" + 0.002*\"great\" + 0.002*\"news\" + 0.002*\"countri\" + 0.002*\"peopl\"\n",
      "Index 1: topic= 0.004*\"dbongino\" + 0.003*\"democrat\" + 0.003*\"great\" + 0.003*\"realdonaldtrump\" + 0.002*\"republican\" + 0.002*\"state\" + 0.002*\"vote\" + 0.002*\"china\" + 0.002*\"work\" + 0.002*\"want\"\n",
      "Index 2: topic= 0.004*\"great\" + 0.003*\"china\" + 0.003*\"trump\" + 0.003*\"time\" + 0.003*\"billion\" + 0.002*\"presid\" + 0.002*\"year\" + 0.002*\"dollar\" + 0.002*\"state\" + 0.002*\"new\"\n"
     ]
    }
   ],
   "source": [
    "for index, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Index {}: topic= {}'.format(index, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "\n",
      "\tStems:\n",
      "\t\trealdonaldtrump\n",
      "\t\tdbongino\n",
      "\t\tpresid\n",
      "\t\ttrump\n",
      "\t\tamerica\n",
      "\t\tchina\n",
      "\t\tgreat\n",
      "\t\tnews\n",
      "\t\tcountri\n",
      "\t\tpeopl\n",
      "\n",
      "\n",
      "\tRecovered Words:\n",
      "\t\trealDonaldTrump\n",
      "\t\tdbongino\n",
      "\t\tPresidential\n",
      "\t\tTrump\n",
      "\t\tAmericans\n",
      "\t\tChina\n",
      "\t\tGreatest\n",
      "\t\tFoxNews\n",
      "\t\tcountries\n",
      "\t\tpeople\n",
      "\n",
      "\n",
      "Topic 1: \n",
      "\n",
      "\tStems:\n",
      "\t\tdbongino\n",
      "\t\tdemocrat\n",
      "\t\tgreat\n",
      "\t\trealdonaldtrump\n",
      "\t\trepublican\n",
      "\t\tstate\n",
      "\t\tvote\n",
      "\t\tchina\n",
      "\t\twork\n",
      "\t\twant\n",
      "\n",
      "\n",
      "\tRecovered Words:\n",
      "\t\tdbongino\n",
      "\t\tDemocrats\n",
      "\t\tGreatest\n",
      "\t\trealDonaldTrump\n",
      "\t\tRepublican\n",
      "\t\tStatement\n",
      "\t\tvote\n",
      "\t\tChina\n",
      "\t\twork\n",
      "\t\twanted\n",
      "\n",
      "\n",
      "Topic 2: \n",
      "\n",
      "\tStems:\n",
      "\t\tgreat\n",
      "\t\tchina\n",
      "\t\ttrump\n",
      "\t\ttime\n",
      "\t\tbillion\n",
      "\t\tpresid\n",
      "\t\tyear\n",
      "\t\tdollar\n",
      "\t\tstate\n",
      "\t\tnew\n",
      "\n",
      "\n",
      "\tRecovered Words:\n",
      "\t\tGreatest\n",
      "\t\tChina\n",
      "\t\tTrump\n",
      "\t\ttime\n",
      "\t\tBillion\n",
      "\t\tPresidential\n",
      "\t\tyears\n",
      "\t\tDollar\n",
      "\t\tStatement\n",
      "\t\tFoxNews\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, topic in lda_model_tfidf.show_topics(-1, formatted=False):\n",
    "    print('Topic {}: '.format(index))\n",
    "    \n",
    "    print('\\n\\tStems:')\n",
    "    \n",
    "    for word, weight in topic:\n",
    "        print('\\t\\t{}'.format(word), end='\\n')\n",
    "#         print('{} '.format(word))\n",
    "\n",
    "    print('\\n\\n\\tRecovered Words:')\n",
    "    \n",
    "    for word, weight in topic:\n",
    "        print('\\t\\t{}'.format(get_original_word(word, raw_docs)), end='\\n')\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
